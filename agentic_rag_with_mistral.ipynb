{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiohappyeyeballs==2.4.3\n",
      "  Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Collecting aiohttp==3.10.8\n",
      "  Using cached aiohttp-3.10.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Collecting aiosignal==1.3.1\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting annotated-types==0.7.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting anyio==4.6.0\n",
      "  Using cached anyio-4.6.0-py3-none-any.whl (89 kB)\n",
      "Collecting arxiv==2.1.3\n",
      "  Using cached arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: asttokens==2.4.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (2.4.1)\n",
      "Collecting async-timeout==4.0.3\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting attrs==24.2.0\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Collecting cachetools==5.5.0\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Collecting certifi==2024.8.30\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Collecting charset-normalizer==3.3.2\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Collecting click==8.1.7\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: comm==0.2.2 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (0.2.2)\n",
      "Collecting dataclasses-json==0.6.7\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: debugpy==1.8.6 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (1.8.6)\n",
      "Requirement already satisfied: decorator==5.1.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (5.1.1)\n",
      "Collecting duckduckgo_search==6.2.13\n",
      "  Using cached duckduckgo_search-6.2.13-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: exceptiongroup==1.2.2 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (1.2.2)\n",
      "Requirement already satisfied: executing==2.1.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (2.1.0)\n",
      "Collecting faiss-cpu==1.8.0.post1\n",
      "  Using cached faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
      "Collecting feedparser==6.0.11\n",
      "  Using cached feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Collecting frozenlist==1.4.1\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.6\n",
      "  Using cached google_ai_generativelanguage-0.6.6-py3-none-any.whl (718 kB)\n",
      "Collecting google-api-core==2.20.0\n",
      "  Using cached google_api_core-2.20.0-py3-none-any.whl (142 kB)\n",
      "Collecting google-api-python-client==2.147.0\n",
      "  Using cached google_api_python_client-2.147.0-py2.py3-none-any.whl (12.2 MB)\n",
      "Collecting google-auth==2.35.0\n",
      "  Using cached google_auth-2.35.0-py2.py3-none-any.whl (208 kB)\n",
      "Collecting google-auth-httplib2==0.2.0\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting google-generativeai==0.7.2\n",
      "  Using cached google_generativeai-0.7.2-py3-none-any.whl (164 kB)\n",
      "Collecting googleapis-common-protos==1.65.0\n",
      "  Using cached googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "Collecting greenlet==3.1.1\n",
      "  Using cached greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n",
      "Collecting grpcio==1.66.2\n",
      "  Using cached grpcio-1.66.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
      "Collecting grpcio-status==1.62.3\n",
      "  Using cached grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
      "Collecting h11==0.14.0\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Collecting httpcore==1.0.6\n",
      "  Using cached httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "Collecting httplib2==0.22.0\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Collecting httpx==0.27.2\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Collecting idna==3.10\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 39)) (6.29.5)\n",
      "Requirement already satisfied: ipython==8.28.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 40)) (8.28.0)\n",
      "Requirement already satisfied: jedi==0.19.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 41)) (0.19.1)\n",
      "Collecting jsonpatch==1.33\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting jsonpointer==3.0.0\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 44)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 45)) (5.7.2)\n",
      "Collecting langchain==0.3.1\n",
      "  Using cached langchain-0.3.1-py3-none-any.whl (1.0 MB)\n",
      "Collecting langchain-community==0.3.1\n",
      "  Using cached langchain_community-0.3.1-py3-none-any.whl (2.4 MB)\n",
      "Collecting langchain-core==0.3.8\n",
      "  Using cached langchain_core-0.3.8-py3-none-any.whl (400 kB)\n",
      "Collecting langchain-google-genai==2.0.0\n",
      "  Using cached langchain_google_genai-2.0.0-py3-none-any.whl (39 kB)\n",
      "Collecting langchain-text-splitters==0.3.0\n",
      "  Using cached langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
      "Collecting langgraph==0.2.34\n",
      "  Using cached langgraph-0.2.34-py3-none-any.whl (107 kB)\n",
      "Collecting langgraph-checkpoint==2.0.0\n",
      "  Using cached langgraph_checkpoint-2.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting langsmith==0.1.130\n",
      "  Using cached langsmith-0.1.130-py3-none-any.whl (294 kB)\n",
      "Collecting marshmallow==3.22.0\n",
      "  Using cached marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 55)) (0.1.7)\n",
      "Collecting msgpack==1.1.0\n",
      "  Using cached msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "Collecting multidict==6.1.0\n",
      "  Using cached multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Collecting mypy-extensions==1.0.0\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 59)) (1.6.0)\n",
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Collecting orjson==3.10.7\n",
      "  Using cached orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "Requirement already satisfied: packaging==24.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 62)) (24.1)\n",
      "Requirement already satisfied: parso==0.8.4 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 63)) (0.8.4)\n",
      "Requirement already satisfied: pexpect==4.9.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 64)) (4.9.0)\n",
      "Requirement already satisfied: platformdirs==4.3.6 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 65)) (4.3.6)\n",
      "Collecting primp==0.6.3\n",
      "  Using cached primp-0.6.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.48 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 67)) (3.0.48)\n",
      "Collecting proto-plus==1.24.0\n",
      "  Using cached proto_plus-1.24.0-py3-none-any.whl (50 kB)\n",
      "Collecting protobuf==4.25.5\n",
      "  Using cached protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Requirement already satisfied: psutil==6.0.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 70)) (6.0.0)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 71)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 72)) (0.2.3)\n",
      "Collecting pyasn1==0.6.1\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Collecting pyasn1_modules==0.4.1\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Collecting pydantic==2.9.2\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Collecting pydantic-settings==2.5.2\n",
      "  Using cached pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
      "Collecting pydantic_core==2.23.4\n",
      "  Using cached pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Requirement already satisfied: Pygments==2.18.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 78)) (2.18.0)\n",
      "Collecting PyMuPDF==1.24.10\n",
      "  Using cached PyMuPDF-1.24.10-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
      "Collecting PyMuPDFb==1.24.10\n",
      "  Using cached PyMuPDFb-1.24.10-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\n",
      "Collecting pyparsing==3.1.4\n",
      "  Using cached pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 82)) (2.9.0.post0)\n",
      "Collecting python-dotenv==1.0.1\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Collecting PyYAML==6.0.2\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "Requirement already satisfied: pyzmq==26.2.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 85)) (26.2.0)\n",
      "Collecting regex==2024.9.11\n",
      "  Using cached regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "Collecting requests==2.32.3\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting requests-toolbelt==1.0.0\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Collecting rsa==4.9\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting sgmllib3k==1.0.0\n",
      "  Using cached sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six==1.16.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 91)) (1.16.0)\n",
      "Collecting sniffio==1.3.1\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting SQLAlchemy==2.0.35\n",
      "  Using cached SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Requirement already satisfied: stack-data==0.6.3 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 94)) (0.6.3)\n",
      "Collecting tenacity==8.5.0\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Collecting tiktoken==0.7.0\n",
      "  Using cached tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: tornado==6.4.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 97)) (6.4.1)\n",
      "Collecting tqdm==4.66.5\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: traitlets==5.14.3 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 99)) (5.14.3)\n",
      "Collecting typing-inspect==0.9.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: typing_extensions==4.12.2 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 101)) (4.12.2)\n",
      "Collecting uritemplate==4.1.1\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting urllib3==2.2.3\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 104)) (0.2.13)\n",
      "Collecting yarl==1.13.1\n",
      "  Using cached yarl-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (447 kB)\n",
      "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'langsmith' candidate (version 0.1.130 at https://files.pythonhosted.org/packages/bf/b0/4c21096c582639a3c949e8e603aa3f53d07104cccdd500153ac3e7135701/langsmith-0.1.130-py3-none-any.whl#sha256=acf27d77e699d84b03045f3f226e78be1dffb3e756aa1a085f9993a45380e8b2 (from https://pypi.org/simple/langsmith/) (requires-python:<4.0,>=3.8.1))\n",
      "Reason for being yanked: Identified some objects that don't serialize well in the optimized form\u001b[0m\u001b[33m\n",
      "\u001b[0mUsing legacy 'setup.py install' for sgmllib3k, since package 'wheel' is not installed.\n",
      "Installing collected packages: sgmllib3k, urllib3, uritemplate, tqdm, tenacity, sniffio, regex, PyYAML, python-dotenv, pyparsing, PyMuPDFb, pydantic_core, pyasn1, protobuf, primp, orjson, numpy, mypy-extensions, multidict, msgpack, marshmallow, jsonpointer, idna, h11, grpcio, greenlet, frozenlist, feedparser, click, charset-normalizer, certifi, cachetools, attrs, async-timeout, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, rsa, requests, PyMuPDF, pydantic, pyasn1_modules, proto-plus, jsonpatch, httplib2, httpcore, googleapis-common-protos, faiss-cpu, duckduckgo_search, anyio, aiosignal, tiktoken, requests-toolbelt, pydantic-settings, httpx, grpcio-status, google-auth, dataclasses-json, arxiv, aiohttp, langsmith, google-auth-httplib2, google-api-core, langchain-core, google-api-python-client, langgraph-checkpoint, langchain-text-splitters, google-ai-generativelanguage, langgraph, langchain, google-generativeai, langchain-google-genai, langchain-community\n",
      "  Running setup.py install for sgmllib3k ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed PyMuPDF-1.24.10 PyMuPDFb-1.24.10 PyYAML-6.0.2 SQLAlchemy-2.0.35 aiohappyeyeballs-2.4.3 aiohttp-3.10.8 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.6.0 arxiv-2.1.3 async-timeout-4.0.3 attrs-24.2.0 cachetools-5.5.0 certifi-2024.8.30 charset-normalizer-3.3.2 click-8.1.7 dataclasses-json-0.6.7 duckduckgo_search-6.2.13 faiss-cpu-1.8.0.post1 feedparser-6.0.11 frozenlist-1.4.1 google-ai-generativelanguage-0.6.6 google-api-core-2.20.0 google-api-python-client-2.147.0 google-auth-2.35.0 google-auth-httplib2-0.2.0 google-generativeai-0.7.2 googleapis-common-protos-1.65.0 greenlet-3.1.1 grpcio-1.66.2 grpcio-status-1.62.3 h11-0.14.0 httpcore-1.0.6 httplib2-0.22.0 httpx-0.27.2 idna-3.10 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.1 langchain-community-0.3.1 langchain-core-0.3.8 langchain-google-genai-2.0.0 langchain-text-splitters-0.3.0 langgraph-0.2.34 langgraph-checkpoint-2.0.0 langsmith-0.1.130 marshmallow-3.22.0 msgpack-1.1.0 multidict-6.1.0 mypy-extensions-1.0.0 numpy-1.26.4 orjson-3.10.7 primp-0.6.3 proto-plus-1.24.0 protobuf-4.25.5 pyasn1-0.6.1 pyasn1_modules-0.4.1 pydantic-2.9.2 pydantic-settings-2.5.2 pydantic_core-2.23.4 pyparsing-3.1.4 python-dotenv-1.0.1 regex-2024.9.11 requests-2.32.3 requests-toolbelt-1.0.0 rsa-4.9 sgmllib3k-1.0.0 sniffio-1.3.1 tenacity-8.5.0 tiktoken-0.7.0 tqdm-4.66.5 typing-inspect-0.9.0 uritemplate-4.1.1 urllib3-2.2.3 yarl-1.13.1\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-04 14:41:44--  https://arxiv.org/pdf/1810.04805.pdf\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.3.42, 151.101.67.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://arxiv.org/pdf/1810.04805 [following]\n",
      "--2024-10-04 14:41:44--  http://arxiv.org/pdf/1810.04805\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 775166 (757K) [application/pdf]\n",
      "Saving to: ‘./data/BERT_arxiv.pdf’\n",
      "\n",
      "./data/BERT_arxiv.p 100%[===================>] 757.00K  3.82MB/s    in 0.2s    \n",
      "\n",
      "2024-10-04 14:41:44 (3.82 MB/s) - ‘./data/BERT_arxiv.pdf’ saved [775166/775166]\n",
      "\n",
      "--2024-10-04 14:41:44--  https://arxiv.org/pdf/2005.11401\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.67.42, 151.101.131.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 885323 (865K) [application/pdf]\n",
      "Saving to: ‘./data/RAG_arxiv.pdf’\n",
      "\n",
      "./data/RAG_arxiv.pd 100%[===================>] 864.57K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-10-04 14:41:44 (6.42 MB/s) - ‘./data/RAG_arxiv.pdf’ saved [885323/885323]\n",
      "\n",
      "--2024-10-04 14:41:45--  https://arxiv.org/pdf/2310.11511\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.195.42, 151.101.3.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1405127 (1.3M) [application/pdf]\n",
      "Saving to: ‘./data/self_rag_arxiv.pdf’\n",
      "\n",
      "./data/self_rag_arx 100%[===================>]   1.34M  8.64MB/s    in 0.2s    \n",
      "\n",
      "2024-10-04 14:41:45 (8.64 MB/s) - ‘./data/self_rag_arxiv.pdf’ saved [1405127/1405127]\n",
      "\n",
      "--2024-10-04 14:41:45--  https://arxiv.org/pdf/2401.15884\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.3.42, 151.101.131.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 643848 (629K) [application/pdf]\n",
      "Saving to: ‘./data/crag_arxiv.pdf’\n",
      "\n",
      "./data/crag_arxiv.p 100%[===================>] 628.76K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-10-04 14:41:45 (6.01 MB/s) - ‘./data/crag_arxiv.pdf’ saved [643848/643848]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "#\n",
    "! wget \"https://arxiv.org/pdf/1810.04805.pdf\" -O ./data/BERT_arxiv.pdf\n",
    "! wget \"https://arxiv.org/pdf/2005.11401\" -O ./data/RAG_arxiv.pdf\n",
    "! wget \"https://arxiv.org/pdf/2310.11511\" -O ./data/self_rag_arxiv.pdf\n",
    "! wget \"https://arxiv.org/pdf/2401.15884\" -O ./data/crag_arxiv.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader,VectorStoreIndex,SummaryIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.tools import FunctionTool,QueryEngineTool\n",
    "from llama_index.core.vector_stores import MetadataFilters,FilterCondition\n",
    "from llama_index.core.llms import MockLLM\n",
    "from typing import List,Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "Document Metadata: {'page_label': '1', 'file_name': 'self_rag_arxiv.pdf', 'file_path': 'data/self_rag_arxiv.pdf', 'file_type': 'application/pdf', 'file_size': 1405127, 'creation_date': '2024-10-04', 'last_modified_date': '2023-10-19'}\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(input_files = ['./data/self_rag_arxiv.pdf']).load_data()\n",
    "print(len(documents))\n",
    "print(f\"Document Metadata: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the documents into chunks/nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of nodes : 43\n",
      "get the content for node 0 :page_label: 1\n",
      "file_name: self_rag_arxiv.pdf\n",
      "file_path: data/self_rag_arxiv.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 1405127\n",
      "creation_date: 2024-10-04\n",
      "last_modified_date: 2023-10-19\n",
      "\n",
      "Preprint.\n",
      "SELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND\n",
      "CRITIQUE THROUGH SELF-REFLECTION\n",
      "Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§\n",
      "†University of Washington§Allen Institute for AI‡IBM Research AI\n",
      "{akari,zeqiuwu,yizhongw,hannaneh }@cs.washington.edu ,avi@us.ibm.com\n",
      "ABSTRACT\n",
      "Despite their remarkable capabilities, large language models (LLMs) often produce\n",
      "responses containing factual inaccuracies due to their sole reliance on the paramet-\n",
      "ric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad\n",
      "hoc approach that augments LMs with retrieval of relevant knowledge, decreases\n",
      "such issues. However, indiscriminately retrieving and incorporating a fixed number\n",
      "of retrieved passages, regardless of whether retrieval is necessary, or passages are\n",
      "relevant, diminishes LM versatility or can lead to unhelpful response generation.\n",
      "We introduce a new framework called Self-Reflective Retrieval-Augmented Gen-\n",
      "eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval\n",
      "and self-reflection. Our framework trains a single arbitrary LM that adaptively\n",
      "retrieves passages on-demand, and generates and reflects on retrieved passages\n",
      "and its own generations using special tokens, called reflection tokens. Generating\n",
      "reflection tokens makes the LM controllable during the inference phase, enabling it\n",
      "to tailor its behavior to diverse task requirements. Experiments show that SELF-\n",
      "RAG(7B and 13B parameters) significantly outperforms state-of-the-art LLMs\n",
      "and retrieval-augmented models on a diverse set of tasks. Specifically, SELF-RAG\n",
      "outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\n",
      "reasoning and fact verification tasks, and it shows significant gains in improving\n",
      "factuality and citation accuracy for long-form generations relative to these models.1\n",
      "1 I NTRODUCTION\n",
      "State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\n",
      "despite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\n",
      "(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\n",
      "with relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\n",
      "2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\n",
      "unnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\n",
      "retrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\n",
      "the output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\n",
      "the models are not explicitly trained to leverage and follow facts from provided passages. This\n",
      "work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an\n",
      "LLM’s generation quality, including its factual accuracy without hurting its versatility, via on-demand\n",
      "retrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\n",
      "its own generation process given a task input by generating both task output and intermittent special\n",
      "tokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to\n",
      "indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\n",
      "given an input prompt and preceding generations, SELF-RAGfirst determines if augmenting the\n",
      "continued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that\n",
      "calls a retriever model on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple\n",
      "retrieved passages, evaluating their relevance and then generating corresponding task outputs (Step\n",
      "2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms\n",
      "of factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which\n",
      "1Our code and trained models are available at https://selfrag.github.io/ .\n",
      "1arXiv:2310.11511v1  [cs.CL]  17 Oct 2023\n"
     ]
    }
   ],
   "source": [
    "splitter = SentenceSplitter(chunk_size=1024,chunk_overlap=100)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "print(f\"Length of nodes : {len(nodes)}\")\n",
    "print(f\"get the content for node 0 :{nodes[0].get_content(metadata_mode='all')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db_mistral\")\n",
    "chroma_collection = db.get_or_create_collection(\"multidocument-agent\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 16033.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "Settings.chunk_size = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!source .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "\n",
    "load_dotenv()\n",
    "# mistral_api_key=  os.environ[\"MISTRAL_API_KEY\"] \n",
    "llm = MistralAI(model=\"mistral-large-latest\",api_key=os.getenv(\"MISTRAL_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate Vectorstore\n",
    "\n",
    "name = \"BERT_arxiv\"\n",
    "vector_index = VectorStoreIndex(nodes,storage_context=storage_context)\n",
    "vector_index.storage_context.vector_store.persist(persist_path=\"/content/chroma_db\")\n",
    "#\n",
    "# Define Vectorstore Autoretrieval tool\n",
    "def vector_query(query:str,page_numbers:Optional[List[str]]=None)->str:\n",
    "  '''\n",
    "  perform vector search over index on\n",
    "  query(str): query string needs to be embedded\n",
    "  page_numbers(List[str]): list of page numbers to be retrieved,\n",
    "                          leave blank if we want to perform a vector search over all pages\n",
    "  '''\n",
    "  page_numbers = page_numbers or []\n",
    "  metadata_dict = [{\"key\":'page_label',\"value\":p} for p in page_numbers]\n",
    "  #\n",
    "  query_engine = vector_index.as_query_engine(llm=MockLLM(),similarity_top_k =2,\n",
    "                                              filters = MetadataFilters.from_dicts(metadata_dict,\n",
    "                                                                                    condition=FilterCondition.OR)\n",
    "                                              )\n",
    "  #\n",
    "  response = query_engine.query(query)\n",
    "  return response\n",
    "#\n",
    "#llamiondex FunctionTool wraps any python function we feed it\n",
    "vector_query_tool = FunctionTool.from_defaults(name=f\"vector_tool_{name}\",\n",
    "                                              fn=vector_query)\n",
    "# Prepare Summary Tool\n",
    "summary_index = SummaryIndex(nodes)\n",
    "# summary_query_engine = summary_index.as_query_engine(llm=MockLLM(), embed_model=\"local\")\n",
    "summary_query_engine = summary_index.as_query_engine(llm=MockLLM(), response_mode=\"tree_summarize\",\n",
    "                                                      se_async=True,)\n",
    "summary_query_tool = QueryEngineTool.from_defaults(name=f\"summary_tool_{name}\",\n",
    "                                                    query_engine=summary_query_engine,\n",
    "                                                  description=(\"Use ONLY IF you want to get a holistic summary of the documents.\"\n",
    "                                              \"DO NOT USE if you have specified questions over the documents.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.predict_and_call([vector_query_tool],\n",
    "                                \"Summarize the content in page number 2\",\n",
    "                                verbose=True)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to generate Vectorstore Tool and Summary tool for all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_tools(file_path:str,name:str)->str:\n",
    "  '''\n",
    "  get vector query and sumnmary query tools from a document\n",
    "  '''\n",
    "  #load documents\n",
    "  documents = SimpleDirectoryReader(input_files = [file_path]).load_data()\n",
    "  print(f\"length of nodes\")\n",
    "  splitter = SentenceSplitter(chunk_size=1024,chunk_overlap=100)\n",
    "  nodes = splitter.get_nodes_from_documents(documents)\n",
    "  print(f\"Length of nodes : {len(nodes)}\")\n",
    "  #instantiate Vectorstore\n",
    "  vector_index = VectorStoreIndex(nodes,storage_context=storage_context)\n",
    "  vector_index.storage_context.vector_store.persist(persist_path=\"/content/chroma_db\")\n",
    "\n",
    "  # Define Vectorstore Autoretrieval tool\n",
    "  def vector_query(query:str,page_numbers:Optional[List[str]]=None)->str:\n",
    "    '''\n",
    "    perform vector search over index on\n",
    "    query(str): query string needs to be embedded\n",
    "    page_numbers(List[str]): list of page numbers to be retrieved,\n",
    "                            leave blank if we want to perform a vector search over all pages\n",
    "    '''\n",
    "    page_numbers = page_numbers or []\n",
    "    metadata_dict = [{\"key\":'page_label',\"value\":p} for p in page_numbers]\n",
    "  \n",
    "    query_engine = vector_index.as_query_engine(llm=MockLLM(),similarity_top_k =2,\n",
    "                                                filters = MetadataFilters.from_dicts(metadata_dict,\n",
    "                                                                                     condition=FilterCondition.OR)\n",
    "                                                )\n",
    "  \n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "  \n",
    "  #llamiondex FunctionTool wraps any python function we feed it\n",
    "  vector_query_tool = FunctionTool.from_defaults(name=f\"vector_tool_{name}\",\n",
    "                                                fn=vector_query)\n",
    "  # Prepare Summary Tool\n",
    "  summary_index = SummaryIndex(nodes)\n",
    "  summary_query_engine = summary_index.as_query_engine(llm=MockLLM(),response_mode=\"tree_summarize\",\n",
    "                                                       se_async=True,)\n",
    "  summary_query_tool = QueryEngineTool.from_defaults(name=f\"summary_tool_{name}\",\n",
    "                                                     query_engine=summary_query_engine,\n",
    "                                                    description=(\"Use ONLY IF you want to get a holistic summary of the documents.\"\n",
    "                                                \"DO NOT USE if you have specified questions over the documents.\"))\n",
    "  return vector_query_tool,summary_query_tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a input list with specified document names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BERT_arxiv', 'self_rag_arxiv', 'crag_arxiv', 'RAG_arxiv']\n",
      "['data/BERT_arxiv.pdf', 'data/self_rag_arxiv.pdf', 'data/crag_arxiv.pdf', 'data/RAG_arxiv.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "root_path = \"data\"\n",
    "file_name = []\n",
    "file_path = []\n",
    "for file in os.listdir(root_path):\n",
    "  if file.endswith(\".pdf\"):\n",
    "    file_name.append(file.split(\".\")[0])\n",
    "    file_path.append(os.path.join(root_path,file))\n",
    "#\n",
    "print(file_name)\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the vectortool and summary tool for each documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of nodes\n",
      "Length of nodes : 28\n",
      "length of nodes\n",
      "Length of nodes : 43\n",
      "length of nodes\n",
      "Length of nodes : 22\n",
      "length of nodes\n",
      "Length of nodes : 30\n"
     ]
    }
   ],
   "source": [
    "papers_to_tools_dict = {}\n",
    "for name,filename in zip(file_name,file_path):\n",
    "  vector_query_tool,summary_query_tool = get_doc_tools(filename,name)\n",
    "  papers_to_tools_dict[name] = [vector_query_tool,summary_query_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the tools into a flat list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<llama_index.core.tools.function_tool.FunctionTool at 0x71c84018afb0>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x71c84017d6f0>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x71c837f39fc0>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x71c837f38850>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x71c840097730>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x71c840097670>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x71c837b76950>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x71c837b74790>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tools = [t for f in file_name for t in papers_to_tools_dict[f]]\n",
    "initial_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(initial_tools,index_cls=VectorStoreIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the ObjectIndex as retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolMetadata(description='Use ONLY IF you want to get a holistic summary of the documents.DO NOT USE if you have specified questions over the documents.', name='summary_tool_self_rag_arxiv', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)\n",
      "ToolMetadata(description='vector_tool_self_rag_arxiv(query: str, page_numbers: Optional[List[str]] = None) -> str\\n\\n    perform vector search over index on\\n    query(str): query string needs to be embedded\\n    page_numbers(List[str]): list of page numbers to be retrieved,\\n                            leave blank if we want to perform a vector search over all pages\\n    ', name='vector_tool_self_rag_arxiv', fn_schema=<class 'llama_index.core.tools.utils.vector_tool_self_rag_arxiv'>, return_direct=False)\n"
     ]
    }
   ],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=2)\n",
    "tools = obj_retriever.retrieve(\"compare and contrast the papers self rag and corrective rag\")\n",
    "#\n",
    "print(tools[0].metadata)\n",
    "print(tools[1].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "#\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(tool_retriever=obj_retriever,\n",
    "                                                     llm=llm,\n",
    "                                                     system_prompt=\"\"\"You are an agent designed to answer queries over a set of given papers.\n",
    "                                                     Please always use the tools provided to answer a question.Do not rely on prior knowledge.\"\"\",\n",
    "                                                     verbose=True)\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.query(\"Compare and contrast self rag and crag.\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Query 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.query(\"Summarize the paper corrective RAG.\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
